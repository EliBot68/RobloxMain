-- DataWarehouseReporting.luau
-- Data warehouse and reporting infrastructure for comprehensive business intelligence
-- Provides ETL processes, data aggregation, and advanced reporting capabilities

local ReplicatedStorage = game:GetService("ReplicatedStorage")
local HttpService = game:GetService("HttpService")
local RunService = game:GetService("RunService")

local SafeRequire = require(ReplicatedStorage.Shared.utils.SafeRequire)

local DataWarehouseReporting = {}

-- ========================================
-- WAREHOUSE CONFIGURATION
-- ========================================

local WAREHOUSE_CONFIG = {
    -- ETL settings
    etl = {
        extractFrequency = 300,           -- 5 minutes
        transformBatchSize = 1000,
        loadFrequency = 600,              -- 10 minutes
        enableRealTimeETL = true,
        enableDataValidation = true,
        enableSchemaEvolution = true,
        retryAttempts = 3,
        timeoutSeconds = 30
    },
    
    -- Data sources
    dataSources = {
        user_events = {
            table = "user_events",
            schema = {
                user_id = "integer",
                event_type = "string",
                timestamp = "timestamp",
                properties = "json"
            },
            partitionBy = "date",
            retentionDays = 365
        },
        
        user_sessions = {
            table = "user_sessions",
            schema = {
                session_id = "string",
                user_id = "integer",
                start_time = "timestamp",
                end_time = "timestamp",
                duration = "integer",
                device_type = "string"
            },
            partitionBy = "date",
            retentionDays = 365
        },
        
        revenue_events = {
            table = "revenue_events",
            schema = {
                transaction_id = "string",
                user_id = "integer",
                amount = "decimal",
                currency = "string",
                product_id = "string",
                timestamp = "timestamp"
            },
            partitionBy = "date",
            retentionDays = 2555 -- 7 years for financial data
        },
        
        user_attributes = {
            table = "user_attributes",
            schema = {
                user_id = "integer",
                attribute_name = "string",
                attribute_value = "string",
                updated_at = "timestamp"
            },
            partitionBy = "user_id",
            retentionDays = 1095 -- 3 years
        }
    },
    
    -- Data models
    dataModels = {
        -- Fact tables
        fact_user_activity = {
            type = "fact",
            grainLevel = "daily",
            dimensions = {"user", "date", "device", "channel"},
            measures = {"sessions", "duration", "events", "revenue"}
        },
        
        fact_revenue = {
            type = "fact",
            grainLevel = "transaction",
            dimensions = {"user", "product", "date", "channel"},
            measures = {"amount", "quantity", "discount"}
        },
        
        -- Dimension tables
        dim_users = {
            type = "dimension",
            attributes = {"user_id", "registration_date", "acquisition_channel", 
                         "device_type", "geography", "segment"}
        },
        
        dim_products = {
            type = "dimension",
            attributes = {"product_id", "product_name", "category", "price", "currency"}
        },
        
        dim_dates = {
            type = "dimension",
            attributes = {"date", "year", "month", "day", "quarter", "week", "day_of_week"}
        }
    },
    
    -- Reporting settings
    reporting = {
        enableScheduledReports = true,
        reportFormats = {"json", "csv", "html", "pdf"},
        defaultFormat = "json",
        maxReportSize = 10 * 1024 * 1024, -- 10MB
        cacheExpiry = 3600,               -- 1 hour
        enableReportArchiving = true
    },
    
    -- Performance settings
    performance = {
        enableIndexing = true,
        enablePartitioning = true,
        enableCompression = true,
        enableCaching = true,
        cacheSize = 100 * 1024 * 1024,    -- 100MB
        queryTimeout = 60,                -- 60 seconds
        maxConcurrentQueries = 10
    }
}

-- ========================================
-- WAREHOUSE STATE
-- ========================================

local WarehouseState = {
    -- ETL components
    extractor = nil,
    transformer = nil,
    loader = nil,
    
    -- Data storage
    rawData = {},
    stagedData = {},
    transformedData = {},
    
    -- Data models
    factTables = {},
    dimensionTables = {},
    
    -- Reporting
    reports = {},
    reportCache = {},
    scheduledReports = {},
    
    -- Metadata
    schema = {},
    dataLineage = {},
    queryHistory = {},
    
    -- Performance
    queryCache = {},
    indexCache = {},
    
    -- Statistics
    stats = {
        totalRecords = 0,
        extractedRecords = 0,
        transformedRecords = 0,
        loadedRecords = 0,
        totalQueries = 0,
        avgQueryTime = 0,
        cacheHitRate = 0,
        dataQualityScore = 100
    }
}

function DataWarehouseReporting.initialize()
    print("üèóÔ∏è Initializing DataWarehouseReporting...")
    
    -- Set up ETL pipeline
    DataWarehouseReporting.setupETLPipeline()
    
    -- Initialize data warehouse
    DataWarehouseReporting.initializeDataWarehouse()
    
    -- Set up reporting engine
    DataWarehouseReporting.setupReportingEngine()
    
    -- Initialize data models
    DataWarehouseReporting.initializeDataModels()
    
    -- Start warehouse operations
    DataWarehouseReporting.startWarehouseOperations()
    
    print("üèóÔ∏è DataWarehouseReporting initialized successfully")
end

-- ========================================
-- ETL PIPELINE
-- ========================================

function DataWarehouseReporting.setupETLPipeline()
    WarehouseState.etlPipeline = {
        extractor = DataWarehouseReporting.createDataExtractor(),
        transformer = DataWarehouseReporting.createDataTransformer(),
        loader = DataWarehouseReporting.createDataLoader(),
        validator = DataWarehouseReporting.createDataValidator(),
        scheduler = DataWarehouseReporting.createETLScheduler()
    }
    
    print("üîÑ ETL pipeline initialized")
end

function DataWarehouseReporting.createDataExtractor()
    return {
        name = "data_extractor",
        
        extract = function(self, source, criteria)
            return DataWarehouseReporting.extractData(source, criteria)
        end,
        
        extractIncremental = function(self, source, lastExtractTime)
            return DataWarehouseReporting.extractIncrementalData(source, lastExtractTime)
        end
    }
end

function DataWarehouseReporting.createDataTransformer()
    return {
        name = "data_transformer",
        
        transform = function(self, data, transformRules)
            return DataWarehouseReporting.transformData(data, transformRules)
        end,
        
        validate = function(self, data, schema)
            return DataWarehouseReporting.validateDataQuality(data, schema)
        end
    }
end

function DataWarehouseReporting.createDataLoader()
    return {
        name = "data_loader",
        
        load = function(self, data, targetTable)
            return DataWarehouseReporting.loadData(data, targetTable)
        end,
        
        upsert = function(self, data, targetTable, keyColumns)
            return DataWarehouseReporting.upsertData(data, targetTable, keyColumns)
        end
    }
end

function DataWarehouseReporting.createDataValidator()
    return {
        name = "data_validator",
        
        validateSchema = function(self, data, schema)
            return DataWarehouseReporting.validateSchema(data, schema)
        end,
        
        validateQuality = function(self, data)
            return DataWarehouseReporting.validateDataQuality(data)
        end
    }
end

function DataWarehouseReporting.createETLScheduler()
    return {
        name = "etl_scheduler",
        jobs = {},
        
        schedule = function(self, job)
            DataWarehouseReporting.scheduleETLJob(job)
        end,
        
        execute = function(self)
            DataWarehouseReporting.executeScheduledJobs()
        end
    }
end

function DataWarehouseReporting.extractData(source, criteria)
    criteria = criteria or {}
    
    local extraction = {
        id = HttpService:GenerateGUID(false),
        source = source,
        criteria = criteria,
        timestamp = tick(),
        status = "running",
        records = {},
        recordCount = 0
    }
    
    -- Simulate data extraction based on source
    if source == "user_events" then
        extraction.records = DataWarehouseReporting.extractUserEvents(criteria)
    elseif source == "user_sessions" then
        extraction.records = DataWarehouseReporting.extractUserSessions(criteria)
    elseif source == "revenue_events" then
        extraction.records = DataWarehouseReporting.extractRevenueEvents(criteria)
    elseif source == "user_attributes" then
        extraction.records = DataWarehouseReporting.extractUserAttributes(criteria)
    end
    
    extraction.recordCount = #extraction.records
    extraction.status = "completed"
    
    -- Store in raw data
    WarehouseState.rawData[extraction.id] = extraction
    
    -- Update statistics
    WarehouseState.stats.extractedRecords = WarehouseState.stats.extractedRecords + extraction.recordCount
    
    print(string.format("üì§ Extracted %d records from %s", extraction.recordCount, source))
    
    return extraction
end

function DataWarehouseReporting.transformData(data, transformRules)
    transformRules = transformRules or {}
    
    local transformation = {
        id = HttpService:GenerateGUID(false),
        inputData = data,
        transformRules = transformRules,
        timestamp = tick(),
        status = "running",
        outputData = {},
        transformedCount = 0,
        errors = {}
    }
    
    -- Apply transformations
    for _, record in ipairs(data.records) do
        local transformedRecord = DataWarehouseReporting.applyTransformations(record, transformRules)
        
        if transformedRecord then
            table.insert(transformation.outputData, transformedRecord)
            transformation.transformedCount = transformation.transformedCount + 1
        else
            table.insert(transformation.errors, {
                record = record,
                error = "Transformation failed"
            })
        end
    end
    
    transformation.status = "completed"
    
    -- Store in staged data
    WarehouseState.stagedData[transformation.id] = transformation
    
    -- Update statistics
    WarehouseState.stats.transformedRecords = WarehouseState.stats.transformedRecords + transformation.transformedCount
    
    print(string.format("üîÑ Transformed %d records (%d errors)", 
        transformation.transformedCount, #transformation.errors))
    
    return transformation
end

function DataWarehouseReporting.loadData(data, targetTable)
    local loading = {
        id = HttpService:GenerateGUID(false),
        data = data,
        targetTable = targetTable,
        timestamp = tick(),
        status = "running",
        loadedCount = 0,
        errors = {}
    }
    
    -- Initialize target table if needed
    if not WarehouseState.transformedData[targetTable] then
        WarehouseState.transformedData[targetTable] = {}
    end
    
    -- Load data into target table
    for _, record in ipairs(data.outputData or data.records) do
        local success, error = DataWarehouseReporting.insertRecord(targetTable, record)
        
        if success then
            loading.loadedCount = loading.loadedCount + 1
        else
            table.insert(loading.errors, {
                record = record,
                error = error
            })
        end
    end
    
    loading.status = "completed"
    
    -- Update statistics
    WarehouseState.stats.loadedRecords = WarehouseState.stats.loadedRecords + loading.loadedCount
    WarehouseState.stats.totalRecords = WarehouseState.stats.totalRecords + loading.loadedCount
    
    print(string.format("üì• Loaded %d records into %s (%d errors)", 
        loading.loadedCount, targetTable, #loading.errors))
    
    return loading
end

function DataWarehouseReporting.runETLJob(sourceName, targetTable)
    print(string.format("üîÑ Running ETL job: %s ‚Üí %s", sourceName, targetTable))
    
    -- Extract
    local extraction = WarehouseState.etlPipeline.extractor:extract(sourceName)
    
    if extraction.recordCount == 0 then
        print("  üì§ No data to extract")
        return true
    end
    
    -- Transform
    local transformRules = DataWarehouseReporting.getTransformRules(sourceName, targetTable)
    local transformation = WarehouseState.etlPipeline.transformer:transform(extraction, transformRules)
    
    -- Validate
    local validation = WarehouseState.etlPipeline.validator:validateQuality(transformation)
    
    if not validation.passed then
        print(string.format("  ‚ùå Data validation failed: %s", validation.error))
        return false
    end
    
    -- Load
    local loading = WarehouseState.etlPipeline.loader:load(transformation, targetTable)
    
    print(string.format("  ‚úÖ ETL job completed: %d records processed", loading.loadedCount))
    
    return true
end

-- ========================================
-- DATA WAREHOUSE
-- ========================================

function DataWarehouseReporting.initializeDataWarehouse()
    WarehouseState.warehouse = {
        tables = {},
        indexes = {},
        partitions = {},
        statistics = {}
    }
    
    -- Create data model tables
    DataWarehouseReporting.createDataModelTables()
    
    -- Create indexes
    DataWarehouseReporting.createIndexes()
    
    print("üèóÔ∏è Data warehouse initialized")
end

function DataWarehouseReporting.createDataModelTables()
    -- Create fact tables
    for tableName, model in pairs(WAREHOUSE_CONFIG.dataModels) do
        if model.type == "fact" then
            WarehouseState.factTables[tableName] = {
                name = tableName,
                model = model,
                data = {},
                indexes = {},
                partitions = {},
                statistics = {
                    rowCount = 0,
                    lastUpdated = tick()
                }
            }
        elseif model.type == "dimension" then
            WarehouseState.dimensionTables[tableName] = {
                name = tableName,
                model = model,
                data = {},
                indexes = {},
                statistics = {
                    rowCount = 0,
                    lastUpdated = tick()
                }
            }
        end
    end
end

function DataWarehouseReporting.createIndexes()
    -- Create indexes for better query performance
    for tableName, table in pairs(WarehouseState.factTables) do
        for _, dimension in ipairs(table.model.dimensions) do
            DataWarehouseReporting.createIndex(tableName, dimension)
        end
    end
    
    for tableName, table in pairs(WarehouseState.dimensionTables) do
        DataWarehouseReporting.createIndex(tableName, "primary_key")
    end
end

function DataWarehouseReporting.createIndex(tableName, columnName)
    local indexName = string.format("idx_%s_%s", tableName, columnName)
    
    WarehouseState.warehouse.indexes[indexName] = {
        tableName = tableName,
        columnName = columnName,
        type = "btree",
        created = tick()
    }
    
    print(string.format("üìä Created index: %s", indexName))
end

function DataWarehouseReporting.insertRecord(tableName, record)
    -- Validate record against schema
    local schema = WAREHOUSE_CONFIG.dataSources[tableName]
    if schema then
        local validation = DataWarehouseReporting.validateSchema(record, schema.schema)
        if not validation.valid then
            return false, validation.error
        end
    end
    
    -- Insert into appropriate table structure
    if not WarehouseState.transformedData[tableName] then
        WarehouseState.transformedData[tableName] = {}
    end
    
    table.insert(WarehouseState.transformedData[tableName], record)
    
    -- Update table statistics
    if WarehouseState.factTables[tableName] then
        WarehouseState.factTables[tableName].statistics.rowCount = 
            WarehouseState.factTables[tableName].statistics.rowCount + 1
    elseif WarehouseState.dimensionTables[tableName] then
        WarehouseState.dimensionTables[tableName].statistics.rowCount = 
            WarehouseState.dimensionTables[tableName].statistics.rowCount + 1
    end
    
    return true
end

function DataWarehouseReporting.queryData(query)
    local queryExecution = {
        id = HttpService:GenerateGUID(false),
        query = query,
        timestamp = tick(),
        startTime = tick(),
        status = "running",
        results = {},
        resultCount = 0,
        executionTime = 0
    }
    
    -- Check cache first
    local cacheKey = DataWarehouseReporting.generateCacheKey(query)
    local cachedResult = WarehouseState.queryCache[cacheKey]
    
    if cachedResult and (tick() - cachedResult.timestamp) < WAREHOUSE_CONFIG.reporting.cacheExpiry then
        queryExecution.results = cachedResult.results
        queryExecution.resultCount = cachedResult.resultCount
        queryExecution.status = "cached"
        queryExecution.executionTime = 0.001 -- Cached queries are very fast
        
        -- Update cache hit rate
        WarehouseState.stats.cacheHitRate = 
            (WarehouseState.stats.cacheHitRate * WarehouseState.stats.totalQueries + 1) / 
            (WarehouseState.stats.totalQueries + 1)
        
        print("üìä Query served from cache")
    else
        -- Execute query
        queryExecution.results = DataWarehouseReporting.executeQuery(query)
        queryExecution.resultCount = #queryExecution.results
        queryExecution.status = "completed"
        queryExecution.executionTime = tick() - queryExecution.startTime
        
        -- Cache result
        WarehouseState.queryCache[cacheKey] = {
            results = queryExecution.results,
            resultCount = queryExecution.resultCount,
            timestamp = tick()
        }
        
        print(string.format("üìä Query executed: %d results in %.3fs", 
            queryExecution.resultCount, queryExecution.executionTime))
    end
    
    -- Store query history
    WarehouseState.queryHistory[queryExecution.id] = queryExecution
    
    -- Update statistics
    WarehouseState.stats.totalQueries = WarehouseState.stats.totalQueries + 1
    WarehouseState.stats.avgQueryTime = 
        (WarehouseState.stats.avgQueryTime * (WarehouseState.stats.totalQueries - 1) + 
         queryExecution.executionTime) / WarehouseState.stats.totalQueries
    
    return queryExecution
end

-- ========================================
-- REPORTING ENGINE
-- ========================================

function DataWarehouseReporting.setupReportingEngine()
    WarehouseState.reportingEngine = {
        generator = DataWarehouseReporting.createReportGenerator(),
        formatter = DataWarehouseReporting.createReportFormatter(),
        scheduler = DataWarehouseReporting.createReportScheduler(),
        distributor = DataWarehouseReporting.createReportDistributor()
    }
    
    print("üìä Reporting engine initialized")
end

function DataWarehouseReporting.createReportGenerator()
    return {
        name = "report_generator",
        
        generate = function(self, reportConfig)
            return DataWarehouseReporting.generateReport(reportConfig)
        end
    }
end

function DataWarehouseReporting.createReportFormatter()
    return {
        name = "report_formatter",
        
        format = function(self, data, format)
            return DataWarehouseReporting.formatReport(data, format)
        end
    }
end

function DataWarehouseReporting.createReportScheduler()
    return {
        name = "report_scheduler",
        
        schedule = function(self, reportConfig)
            DataWarehouseReporting.scheduleReport(reportConfig)
        end
    }
end

function DataWarehouseReporting.createReportDistributor()
    return {
        name = "report_distributor",
        
        distribute = function(self, report, recipients)
            DataWarehouseReporting.distributeReport(report, recipients)
        end
    }
end

function DataWarehouseReporting.generateReport(reportConfig)
    local report = {
        id = HttpService:GenerateGUID(false),
        config = reportConfig,
        timestamp = tick(),
        status = "generating",
        data = {},
        metadata = {
            generatedAt = tick(),
            dataRange = reportConfig.dateRange,
            filters = reportConfig.filters or {}
        }
    }
    
    print(string.format("üìä Generating report: %s", reportConfig.name))
    
    -- Execute report queries
    if reportConfig.type == "user_activity" then
        report.data = DataWarehouseReporting.generateUserActivityReport(reportConfig)
    elseif reportConfig.type == "revenue_summary" then
        report.data = DataWarehouseReporting.generateRevenueSummaryReport(reportConfig)
    elseif reportConfig.type == "retention_cohort" then
        report.data = DataWarehouseReporting.generateRetentionCohortReport(reportConfig)
    elseif reportConfig.type == "kpi_dashboard" then
        report.data = DataWarehouseReporting.generateKPIDashboardReport(reportConfig)
    elseif reportConfig.type == "custom" then
        report.data = DataWarehouseReporting.generateCustomReport(reportConfig)
    end
    
    report.status = "completed"
    
    -- Store report
    WarehouseState.reports[report.id] = report
    
    print(string.format("‚úÖ Report generated: %s (%d data points)", 
        reportConfig.name, DataWarehouseReporting.countReportDataPoints(report.data)))
    
    return report
end

function DataWarehouseReporting.generateUserActivityReport(config)
    local query = {
        type = "aggregate",
        table = "user_sessions",
        groupBy = {"date", "device_type"},
        measures = {
            {name = "sessions", type = "count"},
            {name = "unique_users", type = "count_distinct", column = "user_id"},
            {name = "avg_duration", type = "average", column = "duration"}
        },
        filters = config.filters or {},
        dateRange = config.dateRange
    }
    
    local queryResult = DataWarehouseReporting.queryData(query)
    
    return {
        summary = {
            totalSessions = queryResult.resultCount > 0 and 
                DataWarehouseReporting.sumColumn(queryResult.results, "sessions") or 0,
            uniqueUsers = queryResult.resultCount > 0 and 
                DataWarehouseReporting.sumColumn(queryResult.results, "unique_users") or 0,
            avgSessionDuration = queryResult.resultCount > 0 and 
                DataWarehouseReporting.avgColumn(queryResult.results, "avg_duration") or 0
        },
        breakdown = queryResult.results
    }
end

function DataWarehouseReporting.generateRevenueSummaryReport(config)
    local query = {
        type = "aggregate",
        table = "revenue_events",
        groupBy = {"date", "currency"},
        measures = {
            {name = "revenue", type = "sum", column = "amount"},
            {name = "transactions", type = "count"},
            {name = "paying_users", type = "count_distinct", column = "user_id"}
        },
        filters = config.filters or {},
        dateRange = config.dateRange
    }
    
    local queryResult = DataWarehouseReporting.queryData(query)
    
    return {
        summary = {
            totalRevenue = queryResult.resultCount > 0 and 
                DataWarehouseReporting.sumColumn(queryResult.results, "revenue") or 0,
            totalTransactions = queryResult.resultCount > 0 and 
                DataWarehouseReporting.sumColumn(queryResult.results, "transactions") or 0,
            payingUsers = queryResult.resultCount > 0 and 
                DataWarehouseReporting.sumColumn(queryResult.results, "paying_users") or 0
        },
        breakdown = queryResult.results
    }
end

function DataWarehouseReporting.generateRetentionCohortReport(config)
    -- Simplified cohort analysis
    local cohorts = {}
    
    -- Generate sample cohort data
    for week = 1, 12 do
        local cohort = {
            cohort_week = string.format("2025-W%02d", week),
            cohort_size = math.random(100, 500),
            retention = {}
        }
        
        -- Calculate retention for each week
        for retentionWeek = 1, 8 do
            local retentionRate = math.max(0.1, 1 - (retentionWeek * 0.15) + math.random(-0.05, 0.05))
            cohort.retention[string.format("week_%d", retentionWeek)] = 
                math.floor(cohort.cohort_size * retentionRate)
        end
        
        table.insert(cohorts, cohort)
    end
    
    return {
        cohorts = cohorts,
        analysis = {
            avgRetentionWeek1 = 0.85,
            avgRetentionWeek4 = 0.45,
            avgRetentionWeek8 = 0.25
        }
    }
end

function DataWarehouseReporting.generateKPIDashboardReport(config)
    return {
        kpis = {
            dau = math.random(1000, 5000),
            mau = math.random(10000, 50000),
            arpu = math.random(5, 25),
            retention_7d = math.random(40, 80) / 100,
            conversion_rate = math.random(2, 8) / 100,
            ltv = math.random(20, 100)
        },
        trends = {
            dau_trend = math.random(-10, 10) / 100,
            revenue_trend = math.random(-5, 15) / 100,
            retention_trend = math.random(-5, 5) / 100
        }
    }
end

function DataWarehouseReporting.formatReport(reportData, format)
    format = format or "json"
    
    if format == "json" then
        return HttpService:JSONEncode(reportData)
    elseif format == "csv" then
        return DataWarehouseReporting.convertToCSV(reportData)
    elseif format == "html" then
        return DataWarehouseReporting.convertToHTML(reportData)
    else
        return tostring(reportData)
    end
end

-- ========================================
-- DATA MODELS
-- ========================================

function DataWarehouseReporting.initializeDataModels()
    -- Create sample data for demonstration
    DataWarehouseReporting.createSampleData()
    
    print("üìä Data models initialized with sample data")
end

function DataWarehouseReporting.createSampleData()
    -- Create sample user events
    local userEvents = {}
    for i = 1, 1000 do
        table.insert(userEvents, {
            user_id = math.random(1, 100),
            event_type = {"login", "purchase", "level_up", "social_share"}[math.random(1, 4)],
            timestamp = tick() - math.random(0, 7 * 86400), -- Last 7 days
            properties = {
                device_type = math.random() > 0.5 and "mobile" or "desktop",
                level = math.random(1, 50)
            }
        })
    end
    
    WarehouseState.transformedData.user_events = userEvents
    
    -- Create sample user sessions
    local userSessions = {}
    for i = 1, 500 do
        local startTime = tick() - math.random(0, 7 * 86400)
        local duration = math.random(300, 3600) -- 5 minutes to 1 hour
        
        table.insert(userSessions, {
            session_id = HttpService:GenerateGUID(false),
            user_id = math.random(1, 100),
            start_time = startTime,
            end_time = startTime + duration,
            duration = duration,
            device_type = math.random() > 0.5 and "mobile" or "desktop"
        })
    end
    
    WarehouseState.transformedData.user_sessions = userSessions
    
    -- Create sample revenue events
    local revenueEvents = {}
    for i = 1, 200 do
        table.insert(revenueEvents, {
            transaction_id = HttpService:GenerateGUID(false),
            user_id = math.random(1, 100),
            amount = math.random(5, 50),
            currency = "USD",
            product_id = string.format("product_%d", math.random(1, 10)),
            timestamp = tick() - math.random(0, 30 * 86400) -- Last 30 days
        })
    end
    
    WarehouseState.transformedData.revenue_events = revenueEvents
end

-- ========================================
-- WAREHOUSE OPERATIONS
-- ========================================

function DataWarehouseReporting.startWarehouseOperations()
    spawn(function()
        while true do
            DataWarehouseReporting.performWarehouseOperations()
            task.wait(WAREHOUSE_CONFIG.etl.extractFrequency)
        end
    end)
    
    print("üîÑ Warehouse operations started")
end

function DataWarehouseReporting.performWarehouseOperations()
    -- Run scheduled ETL jobs
    WarehouseState.etlPipeline.scheduler:execute()
    
    -- Perform maintenance tasks
    DataWarehouseReporting.performMaintenance()
    
    -- Update statistics
    DataWarehouseReporting.updateStatistics()
end

function DataWarehouseReporting.performMaintenance()
    -- Clean old query cache
    DataWarehouseReporting.cleanQueryCache()
    
    -- Update table statistics
    DataWarehouseReporting.updateTableStatistics()
    
    -- Optimize indexes
    DataWarehouseReporting.optimizeIndexes()
end

function DataWarehouseReporting.cleanQueryCache()
    local currentTime = tick()
    local expiry = WAREHOUSE_CONFIG.reporting.cacheExpiry
    
    for cacheKey, cachedItem in pairs(WarehouseState.queryCache) do
        if (currentTime - cachedItem.timestamp) > expiry then
            WarehouseState.queryCache[cacheKey] = nil
        end
    end
end

-- ========================================
-- UTILITY FUNCTIONS
-- ========================================

function DataWarehouseReporting.extractUserEvents(criteria)
    -- Simulate extracting user events from source system
    local events = {}
    for i = 1, math.random(50, 200) do
        table.insert(events, {
            user_id = math.random(1, 1000),
            event_type = "sample_event",
            timestamp = tick() - math.random(0, 86400),
            properties = {}
        })
    end
    return events
end

function DataWarehouseReporting.extractUserSessions(criteria)
    -- Simulate extracting user sessions
    local sessions = {}
    for i = 1, math.random(20, 100) do
        table.insert(sessions, {
            session_id = HttpService:GenerateGUID(false),
            user_id = math.random(1, 1000),
            start_time = tick() - math.random(0, 86400),
            duration = math.random(300, 3600)
        })
    end
    return sessions
end

function DataWarehouseReporting.extractRevenueEvents(criteria)
    -- Simulate extracting revenue events
    local events = {}
    for i = 1, math.random(10, 50) do
        table.insert(events, {
            transaction_id = HttpService:GenerateGUID(false),
            user_id = math.random(1, 1000),
            amount = math.random(5, 100),
            timestamp = tick() - math.random(0, 86400)
        })
    end
    return events
end

function DataWarehouseReporting.extractUserAttributes(criteria)
    -- Simulate extracting user attributes
    local attributes = {}
    for i = 1, math.random(30, 150) do
        table.insert(attributes, {
            user_id = math.random(1, 1000),
            attribute_name = "sample_attribute",
            attribute_value = "sample_value",
            updated_at = tick()
        })
    end
    return attributes
end

function DataWarehouseReporting.applyTransformations(record, transformRules)
    -- Apply basic transformations
    local transformed = {}
    
    for key, value in pairs(record) do
        transformed[key] = value
    end
    
    -- Add computed fields
    if record.start_time and record.duration then
        transformed.end_time = record.start_time + record.duration
    end
    
    if record.timestamp then
        transformed.date = math.floor(record.timestamp / 86400)
    end
    
    return transformed
end

function DataWarehouseReporting.getTransformRules(sourceName, targetTable)
    return {
        addComputedFields = true,
        validateSchema = true,
        cleanData = true
    }
end

function DataWarehouseReporting.validateSchema(record, schema)
    local validation = {valid = true, error = nil}
    
    for field, dataType in pairs(schema) do
        local value = record[field]
        
        if not value then
            validation.valid = false
            validation.error = string.format("Missing required field: %s", field)
            return validation
        end
        
        -- Basic type checking
        if dataType == "integer" and type(value) ~= "number" then
            validation.valid = false
            validation.error = string.format("Field %s must be integer", field)
            return validation
        elseif dataType == "string" and type(value) ~= "string" then
            validation.valid = false
            validation.error = string.format("Field %s must be string", field)
            return validation
        end
    end
    
    return validation
end

function DataWarehouseReporting.validateDataQuality(data)
    local validation = {passed = true, error = nil, issues = {}}
    
    -- Check for null values
    local nullCount = 0
    for _, record in ipairs(data.outputData or {}) do
        for key, value in pairs(record) do
            if not value then
                nullCount = nullCount + 1
            end
        end
    end
    
    -- Check data quality threshold
    local totalFields = #(data.outputData or {}) * 5 -- Assume 5 fields per record
    local nullRate = totalFields > 0 and (nullCount / totalFields) or 0
    
    if nullRate > 0.1 then -- 10% null threshold
        validation.passed = false
        validation.error = string.format("High null rate: %.1f%%", nullRate * 100)
    end
    
    return validation
end

function DataWarehouseReporting.executeQuery(query)
    local results = {}
    
    -- Simplified query execution
    if query.type == "aggregate" and query.table then
        local tableData = WarehouseState.transformedData[query.table] or {}
        
        -- Group and aggregate data
        local groups = {}
        
        for _, record in ipairs(tableData) do
            local groupKey = DataWarehouseReporting.buildGroupKey(record, query.groupBy or {})
            
            if not groups[groupKey] then
                groups[groupKey] = {
                    records = {},
                    aggregates = {}
                }
            end
            
            table.insert(groups[groupKey].records, record)
        end
        
        -- Calculate aggregates
        for groupKey, group in pairs(groups) do
            local result = DataWarehouseReporting.parseGroupKey(groupKey, query.groupBy or {})
            
            for _, measure in ipairs(query.measures or {}) do
                result[measure.name] = DataWarehouseReporting.calculateAggregate(
                    group.records, measure.type, measure.column
                )
            end
            
            table.insert(results, result)
        end
    end
    
    return results
end

function DataWarehouseReporting.buildGroupKey(record, groupByFields)
    local keyParts = {}
    
    for _, field in ipairs(groupByFields) do
        table.insert(keyParts, tostring(record[field] or "null"))
    end
    
    return table.concat(keyParts, "|")
end

function DataWarehouseReporting.parseGroupKey(groupKey, groupByFields)
    local result = {}
    local keyParts = string.split(groupKey, "|")
    
    for i, field in ipairs(groupByFields) do
        result[field] = keyParts[i] or "null"
    end
    
    return result
end

function DataWarehouseReporting.calculateAggregate(records, aggregateType, column)
    if aggregateType == "count" then
        return #records
    elseif aggregateType == "sum" and column then
        local sum = 0
        for _, record in ipairs(records) do
            sum = sum + (record[column] or 0)
        end
        return sum
    elseif aggregateType == "average" and column then
        local sum = 0
        local count = 0
        for _, record in ipairs(records) do
            if record[column] then
                sum = sum + record[column]
                count = count + 1
            end
        end
        return count > 0 and (sum / count) or 0
    elseif aggregateType == "count_distinct" and column then
        local unique = {}
        for _, record in ipairs(records) do
            if record[column] then
                unique[record[column]] = true
            end
        end
        local count = 0
        for _ in pairs(unique) do
            count = count + 1
        end
        return count
    end
    
    return 0
end

function DataWarehouseReporting.generateCacheKey(query)
    return HttpService:GenerateGUID(false):sub(1, 8)
end

function DataWarehouseReporting.sumColumn(results, columnName)
    local sum = 0
    for _, result in ipairs(results) do
        sum = sum + (result[columnName] or 0)
    end
    return sum
end

function DataWarehouseReporting.avgColumn(results, columnName)
    if #results == 0 then return 0 end
    
    local sum = DataWarehouseReporting.sumColumn(results, columnName)
    return sum / #results
end

function DataWarehouseReporting.countReportDataPoints(reportData)
    local count = 0
    
    if reportData.breakdown then
        count = count + #reportData.breakdown
    end
    
    if reportData.cohorts then
        count = count + #reportData.cohorts
    end
    
    if reportData.kpis then
        for _ in pairs(reportData.kpis) do
            count = count + 1
        end
    end
    
    return count
end

function DataWarehouseReporting.convertToCSV(data)
    -- Simplified CSV conversion
    return "CSV format data"
end

function DataWarehouseReporting.convertToHTML(data)
    -- Simplified HTML conversion
    return "<html><body>HTML report</body></html>"
end

function DataWarehouseReporting.scheduleETLJob(job)
    table.insert(WarehouseState.etlPipeline.scheduler.jobs, job)
end

function DataWarehouseReporting.executeScheduledJobs()
    -- Execute pending ETL jobs
    for i = #WarehouseState.etlPipeline.scheduler.jobs, 1, -1 do
        local job = table.remove(WarehouseState.etlPipeline.scheduler.jobs, i)
        DataWarehouseReporting.runETLJob(job.source, job.target)
    end
end

function DataWarehouseReporting.scheduleReport(reportConfig)
    table.insert(WarehouseState.scheduledReports, reportConfig)
end

function DataWarehouseReporting.updateStatistics()
    -- Update warehouse statistics
end

function DataWarehouseReporting.updateTableStatistics()
    -- Update table-level statistics
end

function DataWarehouseReporting.optimizeIndexes()
    -- Optimize database indexes
end

-- ========================================
-- PUBLIC API
-- ========================================

function DataWarehouseReporting.runWarehouseDemo()
    print("üèóÔ∏è Running data warehouse and reporting demonstration...")
    
    -- Run ETL pipeline
    print("  üîÑ Running ETL pipeline...")
    
    -- Extract, transform, and load user events
    DataWarehouseReporting.runETLJob("user_events", "fact_user_activity")
    
    -- Extract, transform, and load revenue events
    DataWarehouseReporting.runETLJob("revenue_events", "fact_revenue")
    
    -- Generate reports
    print("  üìä Generating reports...")
    
    -- User activity report
    local userActivityReport = DataWarehouseReporting.generateReport({
        name = "Daily User Activity",
        type = "user_activity",
        dateRange = {
            start = tick() - (7 * 86400),
            end = tick()
        },
        filters = {}
    })
    
    print(string.format("    Generated user activity report: %d data points", 
        DataWarehouseReporting.countReportDataPoints(userActivityReport.data)))
    
    -- Revenue summary report
    local revenueReport = DataWarehouseReporting.generateReport({
        name = "Revenue Summary",
        type = "revenue_summary",
        dateRange = {
            start = tick() - (30 * 86400),
            end = tick()
        },
        filters = {}
    })
    
    print(string.format("    Generated revenue report: %d data points", 
        DataWarehouseReporting.countReportDataPoints(revenueReport.data)))
    
    -- KPI dashboard report
    local kpiReport = DataWarehouseReporting.generateReport({
        name = "KPI Dashboard",
        type = "kpi_dashboard",
        dateRange = {
            start = tick() - (30 * 86400),
            end = tick()
        }
    })
    
    print(string.format("    Generated KPI dashboard: %d metrics", 
        DataWarehouseReporting.countReportDataPoints(kpiReport.data)))
    
    -- Cohort retention report
    local cohortReport = DataWarehouseReporting.generateReport({
        name = "Retention Cohorts",
        type = "retention_cohort",
        dateRange = {
            start = tick() - (90 * 86400),
            end = tick()
        }
    })
    
    print(string.format("    Generated cohort analysis: %d cohorts", 
        DataWarehouseReporting.countReportDataPoints(cohortReport.data)))
    
    -- Test query performance
    print("  ‚ö° Testing query performance...")
    
    local testQuery = {
        type = "aggregate",
        table = "user_sessions",
        groupBy = {"date"},
        measures = {
            {name = "sessions", type = "count"},
            {name = "unique_users", type = "count_distinct", column = "user_id"}
        }
    }
    
    local queryResult = DataWarehouseReporting.queryData(testQuery)
    print(string.format("    Query executed: %d results in %.3fs", 
        queryResult.resultCount, queryResult.executionTime))
    
    -- Test cached query
    local cachedResult = DataWarehouseReporting.queryData(testQuery)
    print(string.format("    Cached query: %d results in %.3fs", 
        cachedResult.resultCount, cachedResult.executionTime))
    
    -- Show warehouse statistics
    local stats = DataWarehouseReporting.getWarehouseStats()
    print("  üìà Data Warehouse Statistics:")
    print(string.format("    Total records: %d", stats.totalRecords))
    print(string.format("    Extracted records: %d", stats.extractedRecords))
    print(string.format("    Transformed records: %d", stats.transformedRecords))
    print(string.format("    Loaded records: %d", stats.loadedRecords))
    print(string.format("    Total queries: %d", stats.totalQueries))
    print(string.format("    Average query time: %.3fs", stats.avgQueryTime))
    print(string.format("    Cache hit rate: %.1f%%", stats.cacheHitRate * 100))
    print(string.format("    Data quality score: %.1f%%", stats.dataQualityScore))
    
    -- Show table information
    print("  üèóÔ∏è Data Warehouse Tables:")
    for tableName, table in pairs(WarehouseState.factTables) do
        print(string.format("    Fact table %s: %d rows", tableName, table.statistics.rowCount))
    end
    
    for tableName, table in pairs(WarehouseState.dimensionTables) do
        print(string.format("    Dimension table %s: %d rows", tableName, table.statistics.rowCount))
    end
    
    print("üèóÔ∏è Data warehouse and reporting demonstration completed")
end

function DataWarehouseReporting.getWarehouseStats()
    return WarehouseState.stats
end

function DataWarehouseReporting.getReport(reportId)
    return WarehouseState.reports[reportId]
end

function DataWarehouseReporting.listReports()
    local reportList = {}
    
    for reportId, report in pairs(WarehouseState.reports) do
        table.insert(reportList, {
            id = reportId,
            name = report.config.name,
            type = report.config.type,
            timestamp = report.timestamp,
            status = report.status
        })
    end
    
    return reportList
end

function DataWarehouseReporting.getQueryHistory()
    return WarehouseState.queryHistory
end

function DataWarehouseReporting.getTableStats(tableName)
    return WarehouseState.factTables[tableName] or WarehouseState.dimensionTables[tableName]
end

-- Export API
DataWarehouseReporting.runETLJob = DataWarehouseReporting.runETLJob
DataWarehouseReporting.generateReport = DataWarehouseReporting.generateReport
DataWarehouseReporting.queryData = DataWarehouseReporting.queryData
DataWarehouseReporting.formatReport = DataWarehouseReporting.formatReport

-- Initialize the data warehouse
DataWarehouseReporting.initialize()

print("üèóÔ∏è DataWarehouseReporting loaded with comprehensive ETL and reporting capabilities")

return DataWarehouseReporting
